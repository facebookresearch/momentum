"use strict";(self.webpackChunkstaticdocs_starter=self.webpackChunkstaticdocs_starter||[]).push([[2673],{48004:(e,r,n)=>{n.r(r),n.d(r,{assets:()=>l,contentTitle:()=>t,default:()=>c,frontMatter:()=>s,metadata:()=>o,toc:()=>d});var a=n(74848),i=n(28453);const s={},t="Visualization using pymomentum rasterizer",o={id:"examples/visualization_pymomentum_rasterizer",title:"Visualization using pymomentum rasterizer",description:"Visualizing results is an important part of what we do. There are various ways to do it, but many of our pipelines rely on rendering MP4 movies in batch for later playback. This works well because videos work in any browser and are easy to share links, etc.",source:"@site/docs_python/02_examples/02_visualization_pymomentum_rasterizer.md",sourceDirName:"02_examples",slug:"/examples/visualization_pymomentum_rasterizer",permalink:"/momentum/pymomentum/examples/visualization_pymomentum_rasterizer",draft:!1,unlisted:!1,editUrl:"https://github.com/facebookresearch/momentum/edit/main/momentum/website/docs_python/02_examples/02_visualization_pymomentum_rasterizer.md",tags:[],version:"current",sidebarPosition:2,frontMatter:{},sidebar:"tutorialSidebar",previous:{title:"Examples",permalink:"/momentum/pymomentum/examples/python_basics"},next:{title:"Development Environment",permalink:"/momentum/pymomentum/developer_guide/development_environment"}},l={},d=[{value:"Why render on the CPU?",id:"why-render-on-the-cpu",level:2},{value:"Why a software rasterizer?",id:"why-a-software-rasterizer",level:2},{value:"What is pymomentum rasterizer?",id:"what-is-pymomentum-rasterizer",level:2},{value:"Using the rasterizer",id:"using-the-rasterizer",level:2},{value:"Getting started",id:"getting-started",level:3},{value:"Cameras",id:"cameras",level:4},{value:"Depth/Image buffers",id:"depthimage-buffers",level:4},{value:"3d primitives",id:"3d-primitives",level:3},{value:"Meshes",id:"meshes",level:4},{value:"Spheres and cylinders",id:"spheres-and-cylinders",level:4},{value:"Transforms",id:"transforms",level:4},{value:"Skeletons",id:"skeletons",level:4},{value:"2d primitives",id:"2d-primitives",level:3},{value:"Rendering on top of existing images",id:"rendering-on-top-of-existing-images",level:3},{value:"Using depth offset for clearer skeleton/keypoint rendering",id:"using-depth-offset-for-clearer-skeletonkeypoint-rendering",level:3},{value:"Ground plane shadows",id:"ground-plane-shadows",level:3},{value:"Generating a video",id:"generating-a-video",level:3},{value:"Multithreading",id:"multithreading",level:3},{value:"Subdivision",id:"subdivision",level:3},{value:"Other buffers",id:"other-buffers",level:3},{value:"Antialiasing",id:"antialiasing",level:3}];function h(e){const r={code:"code",h1:"h1",h2:"h2",h3:"h3",h4:"h4",header:"header",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,i.R)(),...e.components};return(0,a.jsxs)(a.Fragment,{children:[(0,a.jsx)(r.header,{children:(0,a.jsx)(r.h1,{id:"visualization-using-pymomentum-rasterizer",children:"Visualization using pymomentum rasterizer"})}),"\n",(0,a.jsx)(r.p,{children:"Visualizing results is an important part of what we do. There are various ways to do it, but many of our pipelines rely on rendering MP4 movies in batch for later playback. This works well because videos work in any browser and are easy to share links, etc."}),"\n",(0,a.jsx)(r.h2,{id:"why-render-on-the-cpu",children:"Why render on the CPU?"}),"\n",(0,a.jsx)(r.p,{children:"The obvious choice for visualization for anyone with a graphics background is to render using a GPU, which will be super-fast and provide instant feedback. However, this is often a bad idea on the cluster because cloud GPUs are expensive and use significant energy, so using these GPUs to render images can be quite wasteful. Regardless, almost any rendering job using a sufficiently fast rasterizer (such as our own) is more likely to be bottlenecked by I/O bandwidth than rendering speed so you would see minimal speed gains from even the fastest GPU."}),"\n",(0,a.jsx)(r.h2,{id:"why-a-software-rasterizer",children:"Why a software rasterizer?"}),"\n",(0,a.jsx)(r.p,{children:"There are many options for visualizing data, and it is extremely nonobvious why we would want to have our own software rasterizer."}),"\n",(0,a.jsxs)(r.ul,{children:["\n",(0,a.jsxs)(r.li,{children:[(0,a.jsx)(r.strong,{children:"Software or hardware OpenGL"}),": challenging to work with, little error checking, many silent failures that generate missing output. Global state is challenging to manage. Doesn't support arbitrary camera models unless you implement custom shaders."]}),"\n",(0,a.jsxs)(r.li,{children:[(0,a.jsx)(r.strong,{children:"Blender/professional tools"}),": Can produce very pretty soft shadows etc but challenging to work with on the cluster and don't natively support fisheye camera models unless you build a complicated lens shader."]}),"\n",(0,a.jsxs)(r.li,{children:[(0,a.jsx)(r.strong,{children:"WebGL-based visualization"}),": Very nice when working in Jupyter notebooks but totally unsuitable for rendering in batch. Doesn't natively support our camera models OR standard OpenGL shaders so properly rendering with our camera models can be extremely challenging."]}),"\n",(0,a.jsxs)(r.li,{children:[(0,a.jsx)(r.strong,{children:"Pytorch3d"}),": optimized for differentiable rendering, very slow on CPU."]}),"\n"]}),"\n",(0,a.jsx)(r.h2,{id:"what-is-pymomentum-rasterizer",children:"What is pymomentum rasterizer?"}),"\n",(0,a.jsx)(r.p,{children:"Pymomentum rasterizer is a fully-featured rasterizer."}),"\n",(0,a.jsxs)(r.ul,{children:["\n",(0,a.jsxs)(r.li,{children:[(0,a.jsx)(r.strong,{children:"Cross-platform"}),": Implemented using drjit's SIMD wrappers so it runs on both Intel and ARM."]}),"\n",(0,a.jsxs)(r.li,{children:[(0,a.jsx)(r.strong,{children:"Fast"}),": Runs roughly 2x faster than MesaGL's software OpenGL emulation."]}),"\n",(0,a.jsxs)(r.li,{children:[(0,a.jsx)(r.strong,{children:"Threadsafe"}),": releases the Python GIL so you can easily run it multithreaded (e.g. render multiple images or multiple frames at once). Compare with e.g. OpenGL which has tons of internal state."]}),"\n",(0,a.jsxs)(r.li,{children:["Full ",(0,a.jsx)(r.strong,{children:"per-pixel lighting and shading"})," with multiple lights."]}),"\n",(0,a.jsx)(r.li,{children:"Runs completely on the CPU: zero OpenGL or GPU dependencies."}),"\n",(0,a.jsxs)(r.li,{children:[(0,a.jsx)(r.strong,{children:"Easy to use"}),", completely functional interface (no global state as in OpenGL). Good error reporting (e.g. makes sure your indices are reasonable) and sensible defaults (e.g. if you you don't provide a light, a default lighting setup is automatically provided instead of rendering a black frame). Simple to use (just a single import statement)."]}),"\n",(0,a.jsxs)(r.li,{children:["Basic support for ",(0,a.jsx)(r.strong,{children:"texture mapping"}),"."]}),"\n",(0,a.jsx)(r.li,{children:"Support for ground plane shadows."}),"\n",(0,a.jsx)(r.li,{children:"Can render per-pixel triangle or vertex IDs."}),"\n",(0,a.jsx)(r.li,{children:"Supports arbitrary camera models, provided you provide an implementation compliant with the interface in momentum/renderer/camera.h."}),"\n",(0,a.jsx)(r.li,{children:"Includes 2d primitives (lines, circles) as well with depth buffer support."}),"\n"]}),"\n",(0,a.jsx)(r.h2,{id:"using-the-rasterizer",children:"Using the rasterizer"}),"\n",(0,a.jsx)(r.h3,{id:"getting-started",children:"Getting started"}),"\n",(0,a.jsx)(r.h4,{id:"cameras",children:"Cameras"}),"\n",(0,a.jsxs)(r.p,{children:["The first thing you need is a camera. Pymomentum's rasterizer uses the ",(0,a.jsx)(r.code,{children:"pymomentum.renderer.Camera"})," class. You can construct one in a few ways:"]}),"\n",(0,a.jsxs)(r.ol,{children:["\n",(0,a.jsx)(r.li,{children:"You can construct a camera with an intrinsics model and optional extrinsics matrix."}),"\n",(0,a.jsxs)(r.li,{children:["If you have a pymomentum body model, you can use ",(0,a.jsx)(r.code,{children:"pymomentum.renderer.build_cameras_for_body()"})," to create a camera that looks at the body and frames it in view."]}),"\n",(0,a.jsxs)(r.li,{children:["You can construct a default camera and set the extrinsics matrix explicitly, using e.g. ",(0,a.jsx)(r.code,{children:"camera.look_at()"}),"."]}),"\n",(0,a.jsxs)(r.li,{children:["You can use ",(0,a.jsx)(r.code,{children:"camera.frame()"})," to frame a set of 3d points in view (this can be handy for ensuring that an entire animation stays in frame)."]}),"\n"]}),"\n",(0,a.jsxs)(r.p,{children:["Note that the camera determines the image resolution, you can always use ",(0,a.jsx)(r.code,{children:"camera.upsample()"})," to scale up the image as needed for better quality."]}),"\n",(0,a.jsx)(r.pre,{children:(0,a.jsx)(r.code,{className:"language-python",children:"import pymomentum.renderer as pym_renderer\nimport numpy as np\n\nimage_height, image_width = 800, 1000\n\n# Create a pinhole intrinsics model\nintrinsics = pym_renderer.PinholeIntrinsicsModel(\n    image_width=image_width,\n    image_height=image_height,\n    fx=800.0,  # focal length in pixels\n    fy=800.0,\n    cx=image_width / 2.0,  # principal point\n    cy=image_height / 2.0\n)\n\n# Create a camera with the intrinsics\ncamera = pym_renderer.Camera(intrinsics)\n\n# Move the camera along -z and look at the origin\ncamera = camera.look_at(\n    position=np.array([0, 0, 1]), target=np.zeros(3), up=np.array([0, 1, 0])\n)\n\n# Make sure the entire object is in view:\ncamera = camera.frame(vertex_positions)\n"})}),"\n",(0,a.jsx)(r.h4,{id:"depthimage-buffers",children:"Depth/Image buffers"}),"\n",(0,a.jsx)(r.p,{children:"Now you need to create depth and RGB buffers to render onto. This is very easy now that you have a camera."}),"\n",(0,a.jsx)(r.pre,{children:(0,a.jsx)(r.code,{className:"language-python",children:"import pymomentum.renderer as pym_renderer\nz_buffer = pym_renderer.create_z_buffer(camera)\nrgb_buffer = pym_renderer.create_rgb_buffer(camera)\n"})}),"\n",(0,a.jsx)(r.p,{children:"Note: the buffer size will get padded out to the nearest multiple of 8 for better SIMD performance. You can correct this after the rendering is complete using standard slicing:"}),"\n",(0,a.jsx)(r.pre,{children:(0,a.jsx)(r.code,{className:"language-python",children:"z_buffer = z_buffer[:,:camera.image_width]\nrgb_buffer = rgb_buffer[:,:camera.image_height]\n"})}),"\n",(0,a.jsx)(r.h3,{id:"3d-primitives",children:"3d primitives"}),"\n",(0,a.jsx)(r.h4,{id:"meshes",children:"Meshes"}),"\n",(0,a.jsx)(r.p,{children:"Now, rasterizing a mesh onto the image is a single function call."}),"\n",(0,a.jsx)(r.pre,{children:(0,a.jsx)(r.code,{className:"language-python",children:"pym_renderer.rasterize_mesh(vertex_positions,\n  vertex_normals, triangles, camera, z_buffer=z_buffer, rgb_buffer=rgb_buffer)\n"})}),"\n",(0,a.jsxs)(r.p,{children:["If you have multiple meshes to render, you just call ",(0,a.jsx)(r.code,{children:"rasterize_mesh"})," repeatedly using the same z_buffer."]}),"\n",(0,a.jsx)(r.p,{children:"There is a special function to simplify rasterizing posed pymomentum Characters that takes in a skeleton state:"}),"\n",(0,a.jsx)(r.pre,{children:(0,a.jsx)(r.code,{className:"language-python",children:"skel_state = pym_geometry.model_parameters_to_skeleton_state(character, model_params)\npym_renderer.rasterize_character(character, skel_state, camera, z_buffer, rgb_buffer)\n"})}),"\n",(0,a.jsx)(r.p,{children:"The default render uses a basic material (white diffuse) and a basic but usable lighting setup where the light is co-located with the camera. If you want a shinier setup, you can change the material:"}),"\n",(0,a.jsx)(r.pre,{children:(0,a.jsx)(r.code,{className:"language-python",children:"mat = pym_renderer.PhongMaterial(diffuse_color=np.array([0.8, 0.9, 1.0]),\n    specular_color=np.ones(3) * 0.3)\n\npym_renderer.rasterize_mesh(vertex_positions,\n  vertex_normals, triangles, camera, z_buffer=z_buffer, rgb_buffer=rgb_buffer,\n  material=mat)\n"})}),"\n",(0,a.jsx)(r.p,{children:"If you want to render a wireframe on your mesh, you can use this command:"}),"\n",(0,a.jsx)(r.pre,{children:(0,a.jsx)(r.code,{className:"language-python",children:"pym_renderer.rasterize_wireframe(vertex_positions,\n  triangles, camera, z_buffer=z_buffer, rgb_buffer=rgb_buffer)\n"})}),"\n",(0,a.jsx)(r.h4,{id:"spheres-and-cylinders",children:"Spheres and cylinders"}),"\n",(0,a.jsx)(r.p,{children:"There is special functionality for rendering spheres and cylinders."}),"\n",(0,a.jsx)(r.pre,{children:(0,a.jsx)(r.code,{className:"language-python",children:"sphere_centers = torch.stack(\n    [torch.arange(-10, 10, 3), 5 * torch.ones(7), torch.ones(7)]\n).transpose(0, 1)\npym_renderer.rasterize_spheres(\n    sphere_centers, camera, z_buffer, rgb_buffer=rgb_buffer, radius=torch.ones(7)\n)\npym_renderer.rasterize_cylinders(\n    start_position=torch.tensor([[-5, 8, 0]]),\n    end_position=torch.tensor([[5, 8, 0]]),\n    camera=camera,\n    z_buffer=z_buffer,\n    rgb_buffer=rgb_buffer\n)\n"})}),"\n",(0,a.jsx)(r.p,{children:"You also generate a nice checkerboard ground plane (y defaults to up, but you can change this with the model_matrix if needed)."}),"\n",(0,a.jsx)(r.pre,{children:(0,a.jsx)(r.code,{className:"language-python",children:"pym_renderer.rasterize_checkerboard(\n    camera=camera,\n    z_buffer=z_buffer,\n    rgb_buffer=rgb_buffer,\n)\n"})}),"\n",(0,a.jsx)(r.h4,{id:"transforms",children:"Transforms"}),"\n",(0,a.jsx)(r.p,{children:"You can also transform any object by passing a model transform, the rasterizer is capable of dealing with nonuniform scale and shearing:"}),"\n",(0,a.jsx)(r.pre,{children:(0,a.jsx)(r.code,{className:"language-python",children:"xf = np.array(\n    [[1, 0, 0, 0], [0, 0.3, 0, 5], [0, 0, 1, 0], [0, 0, 0, 1]], dtype=np.float32\n)\n\npym_renderer.rasterize_mesh(vertex_positions,\n  vertex_normals, triangles, camera, z_buffer=z_buffer, rgb_buffer=rgb_buffer,\n  material=mat, model_matrix=xf)\n"})}),"\n",(0,a.jsx)(r.h4,{id:"skeletons",children:"Skeletons"}),"\n",(0,a.jsx)(r.p,{children:"Because Character skeletons are so important to working with momentum, we have some extra functionality for rendering them."}),"\n",(0,a.jsx)(r.pre,{children:(0,a.jsx)(r.code,{className:"language-python",children:"pym_renderer.rasterize_skeleton(character, skel_state, camera, z_buffer,\n    rgb_buffer=rgb_buffer, style=pym_renderer.SkeletonStyle.Pipes,\n    image_offset=np.asarray([-600, 0]), sphere_radius=1.0, cylinder_radius=0.5)\npym_renderer.rasterize_skeleton(character, skel_state, camera, z_buffer,\n    rgb_buffer=rgb_buffer, style=pym_renderer.SkeletonStyle.Octahedrons,\n    sphere_radius=1.0)\npym_renderer.rasterize_skeleton(character, skel_state, camera, z_buffer,\n    rgb_buffer=rgb_buffer, style=pym_renderer.SkeletonStyle.Lines,\n    image_offset=np.asarray([600, 0]), sphere_radius=5.0, cylinder_radius=2.0,\n    sphere_material=pym_renderer.PhongMaterial(np.asarray([1, 0.6, 0.6])))\n"})}),"\n",(0,a.jsx)(r.p,{children:'There are three different skeleton "styles": "Pipes" (3d cylinders and spheres), "Octahedrons" (this asymmetric octahedron shape, useful for visualizing rotations) and "Lines" (2d lines and circles).'}),"\n",(0,a.jsx)(r.h3,{id:"2d-primitives",children:"2d primitives"}),"\n",(0,a.jsx)(r.p,{children:"It can be useful to render 2d primitives like circles and lines but have them respect the z buffer. This can be used:"}),"\n",(0,a.jsxs)(r.ol,{children:["\n",(0,a.jsx)(r.li,{children:"To create a nice grid on the ground plane."}),"\n",(0,a.jsx)(r.li,{children:"To render e.g. 3d keypoints (also see the note below about using a depth offset)."}),"\n"]}),"\n",(0,a.jsx)(r.p,{children:"Now, spheres and cylinders work pretty well for these needs, but (1) lines and circles are significantly faster since approximating a sphere requires >100 triangles (2) lines and circles have radius/thickness values defined in pixels instead of worldspace units, making tuning their size easier (3) lines and circles can look more aesthetically pleasing depending on the use case."}),"\n",(0,a.jsx)(r.pre,{children:(0,a.jsx)(r.code,{className:"language-python",children:"pym_renderer.rasterize_lines(\n    positions=line_positions,\n    camera=camera,\n    z_buffer=z_buffer,\n    rgb_buffer=rgb_buffer,\n    thickness=2.0,\n    color=np.array([1.0, 0.0, 0.0])  # Red lines\n)\npym_renderer.rasterize_circles(\n    positions=circle_positions,\n    camera=camera,\n    z_buffer=z_buffer,\n    rgb_buffer=rgb_buffer,\n    radius=5.0,\n    line_thickness=1.0,\n    line_color=np.array([0.0, 1.0, 0.0])  # Green circles\n)\n"})}),"\n",(0,a.jsx)(r.p,{children:"Note that aliasing can be particularly bad for lines so see the notes about antialiasing below."}),"\n",(0,a.jsx)(r.h3,{id:"rendering-on-top-of-existing-images",children:"Rendering on top of existing images"}),"\n",(0,a.jsxs)(r.p,{children:["If you want to render on top of an existing image, you can use the ",(0,a.jsx)(r.code,{children:"alpha_matte"})," function. This will automatically downsample the image if necessary (if it was upsampled for anti-aliasing reasons) and handle conversions between float- and uint8-valued buffers."]}),"\n",(0,a.jsx)(r.pre,{children:(0,a.jsx)(r.code,{className:"language-python",children:"import cv2\n\ntgt_image = cv2.imread(...)\n# OpenCV likes to use BGR but we use RGB\ntgt_image = tgt_image[..., ::-1]\n\nrgb_buffer = pym_renderer.create_rgb_buffer(camera)\nz_buffer = pym_renderer.create_z_buffer(camera)\n\n# Target image is a [height x width x 3] float- or uint8-valued array:\npym_renderer.alpha_matte(z_buffer, rgb_buffer, tgt_image)\n"})}),"\n",(0,a.jsx)(r.h3,{id:"using-depth-offset-for-clearer-skeletonkeypoint-rendering",children:"Using depth offset for clearer skeleton/keypoint rendering"}),"\n",(0,a.jsx)(r.p,{children:"A classic approach to rendering e.g. 3d keypoints is to render circles on top of the image. The problem with this approach is that because the depth buffer is not respected, the keypoints will be visible through the mesh. This can be very confusing to look at. Notice in the left character how the skeleton of the right hand is visible all the way through the character, which makes it hard to see what is going on."}),"\n",(0,a.jsxs)(r.p,{children:["We can use the z buffer to correct this, but if we try to rasterize the skeleton to the same image where the mesh is the skeleton will be completely hidden. Passing a ",(0,a.jsx)(r.code,{children:"depth_offset"})," to the rasterizer bumps the skeleton forward, allowing you to see the parts of the skeleton that are just below the mesh surface but still hiding parts of the skeleton that are far behind (the right image)."]}),"\n",(0,a.jsx)(r.pre,{children:(0,a.jsx)(r.code,{className:"language-python",children:'pym_renderer.rasterize_character(character, skel_state, camera, z_buffer,\n    rgb_buffer=rgb_buffer, image_offset=np.asarray([300, 0]),\n    material=pym_renderer.PhongMaterial(np.asarray([1, 0.6, 0.6])))\n# Use depth_offset to bump the skeleton forward so we can see it "through" the mesh:\npym_renderer.rasterize_skeleton(character, skel_state, camera, z_buffer,\n    rgb_buffer=rgb_buffer, style=pym_renderer.SkeletonStyle.Pipes, sphere_radius=1.0,\n    cylinder_radius=0.5, depth_offset=-15, image_offset=np.asarray([300, 0]))\n'})}),"\n",(0,a.jsxs)(r.p,{children:["In addition, you can pass an ",(0,a.jsx)(r.code,{children:"image_offset"})," (in pixels) to any rasterizer function and it will displace a mesh in image-space."]}),"\n",(0,a.jsx)(r.pre,{children:(0,a.jsx)(r.code,{className:"language-python",children:"# Render body and skeleton side-by-side using image_offset:\npym_renderer.rasterize_character(character, skel_state, camera, z_buffer,\n    rgb_buffer=rgb_buffer, image_offset=np.asarray([-300, 0]),\n    material=pym_renderer.PhongMaterial(np.asarray([1, 0.6, 0.6])))\npym_renderer.rasterize_skeleton(character, skel_state, camera, z_buffer,\n    rgb_buffer=rgb_buffer, style=pym_renderer.SkeletonStyle.Octahedrons,\n    sphere_radius=1.0, cylinder_radius=0.5, depth_offset=-15,\n    image_offset=np.asarray([300, 0]))\n"})}),"\n",(0,a.jsx)(r.h3,{id:"ground-plane-shadows",children:"Ground plane shadows"}),"\n",(0,a.jsx)(r.p,{children:"Shadows can be very helpful in debugging lower body motion. The rasterizer does not support fully general shadows but there is a basic old-school OpenGL trick you can use to generate a nice shadow on the ground plane."}),"\n",(0,a.jsx)(r.p,{children:"Basically, we can rasterize the mesh projected down onto the ground plane. This is done as a two-step process: the first rasterizes the mesh, generating a depth buffer, and the second splats this shadow onto the ground."}),"\n",(0,a.jsx)(r.pre,{children:(0,a.jsx)(r.code,{className:"language-python",children:"# Two lights, the first is above the person and casts shadows while the other\n# is co-located with the camera to ensure good fill.\nlights = [pym_renderer.Light.create_point_light(\n        np.asarray([-20, 200, 30]), color=np.asarray([0.7, 0.7, 0.7])\n    ), pym_renderer.Light.create_point_light(\n        camera.center_of_projection,\n        np.asarray([0.3, 0.3, 0.3]),\n    )]\n\n# Create a separate z buffer for the shadows.\nshadow_buffer = pym_renderer.create_z_buffer(camera)\n# Rasterize the body mesh onto the shadow Z buffer using a projection matrix\n# constructed from the first light:\npym_renderer.rasterize_character(\n    character,\n    skel_state,\n    camera,\n    z_buffer=shadow_buffer,\n    model_matrix=pym_renderer.create_shadow_projection_matrix(lights[0]),\n    back_face_culling=False,  # Disable back-face culling in case the project inverts triangles.\n)\n\n# Rasterizer the ground plane to our RGB buffer:\npym_renderer.rasterize_checkerboard(camera, z_buffer, rgb_buffer, width=500,\n    subdivisions=3)\n# Use the shadow z buffer to darken the ground plane wherever the shadow hits:\nvery_far = 10000.0\nrgb_buffer *= (\n    torch.logical_or(shadow_buffer > very_far, z_buffer > very_far)\n    .to(torch.float)\n    .clamp(0.5, 1.0)\n    .unsqueeze(-1)\n)\n# Finally rasterize the character mesh:\npym_renderer.rasterize_character(\n    character, skel_state, camera, z_buffer, rgb_buffer=rgb_buffer, lights=lights\n)\n"})}),"\n",(0,a.jsx)(r.h3,{id:"generating-a-video",children:"Generating a video"}),"\n",(0,a.jsx)(r.p,{children:"For video generation, you can use standard video writing libraries like OpenCV or ffmpeg-python. The basic idea is to render each frame to a buffer, and then write the buffer to a video file."}),"\n",(0,a.jsx)(r.pre,{children:(0,a.jsx)(r.code,{className:"language-python",children:"import cv2\n\n# Initialize video writer\nfourcc = cv2.VideoWriter_fourcc(*'mp4v')\nvideo_writer = cv2.VideoWriter(file_path, fourcc, video_fps, (video_width, video_height))\n\nfor i_frame in range(n_frames):\n    full_image = np.zeros(shape=(video_height, video_width, 3), dtype=np.uint8)\n    # ... render your frame ...\n    # Convert RGB to BGR for OpenCV\n    bgr_image = full_image[..., ::-1]\n    video_writer.write(bgr_image)\n\nvideo_writer.release()\n"})}),"\n",(0,a.jsx)(r.h3,{id:"multithreading",children:"Multithreading"}),"\n",(0,a.jsxs)(r.p,{children:["As noted above, ",(0,a.jsx)(r.code,{children:"pymomentum.renderer"})," works well in a multithreaded setting. The simplest way to leverage this is using ",(0,a.jsx)(r.code,{children:"multiprocessing.dummy.Pool()"}),":"]}),"\n",(0,a.jsx)(r.pre,{children:(0,a.jsx)(r.code,{className:"language-python",children:'def rasterize_one_frame(frame_idx: int):\n    # ... your rendering code here ...\n    return rendered_image\n\nn_threads = 4\nfourcc = cv2.VideoWriter_fourcc(*\'mp4v\')\nvideo_writer = cv2.VideoWriter(\n    os.path.join(out_path, "animation.mp4"),\n    fourcc, 30, (image_width, image_height)\n)\n\nwith multiprocessing.dummy.Pool(n_threads) as pool:\n    for idx, image in enumerate(\n        pool.imap(rasterize_one_frame, frames_to_write)\n    ):\n        # Convert RGB to BGR for OpenCV\n        bgr_image = image[..., ::-1]\n        video_writer.write(bgr_image)\n\n        if idx % 10 == 0:\n            print(f"Write frame {idx} of {len(frames_to_write)}")\n\nvideo_writer.release()\n'})}),"\n",(0,a.jsx)(r.p,{children:"Typically the speedup you get is bottlenecked by the serial parts (video encoding, Python overhead) so you won't see a perfectly linear speedup, but in the above code I saw a roughly 3x speedup on 4 threads (60s-20s) and a 4x speedup on 8 threads (60s-15s). Note that this is a sequence of 675 frames (with shadows and 2x supersampling) and we are getting ~45fps on 8 threads."}),"\n",(0,a.jsx)(r.h3,{id:"subdivision",children:"Subdivision"}),"\n",(0,a.jsxs)(r.p,{children:["One potential drawback of rasterizing is that we only apply camera distortion to vertices, and the interpolation between vertices is linear. If you have very large objects, you will start to notice that triangles aren't \"bending\" the way you'd expect toward the edge of wide-angle cameras. The way to address this is to break the mesh into smaller triangles, and ",(0,a.jsx)(r.code,{children:"pymomentum.renderer"})," provides functionality to do this with the ",(0,a.jsx)(r.code,{children:"subdivide_mesh"})," function:"]}),"\n",(0,a.jsx)(r.pre,{children:(0,a.jsx)(r.code,{className:"language-python",children:"subdivided_vertices, subdivided_normals, subdivided_triangles, _, _ = pym_renderer.subdivide_mesh(\n    vertices=vertex_positions,\n    normals=vertex_normals,\n    triangles=triangles,\n    levels=2,  # Number of subdivision levels\n    max_edge_length=10.0  # Maximum edge length before subdivision\n)\n"})}),"\n",(0,a.jsx)(r.p,{children:"The function will subdivide triangles based on:"}),"\n",(0,a.jsxs)(r.ol,{children:["\n",(0,a.jsxs)(r.li,{children:[(0,a.jsx)(r.strong,{children:"levels"}),": Number of subdivision iterations to perform"]}),"\n",(0,a.jsxs)(r.li,{children:[(0,a.jsx)(r.strong,{children:"max_edge_length"}),": Maximum allowed edge length - longer edges will be broken into smaller triangles."]}),"\n"]}),"\n",(0,a.jsx)(r.h3,{id:"other-buffers",children:"Other buffers"}),"\n",(0,a.jsx)(r.p,{children:"The rasterizer knows how to render other quantities as well."}),"\n",(0,a.jsxs)(r.ul,{children:["\n",(0,a.jsxs)(r.li,{children:["The ",(0,a.jsx)(r.code,{children:"vertex_index_buffer"})," rasterizes the index of the vertex to the buffer, or -1 for empty pixels."]}),"\n",(0,a.jsxs)(r.li,{children:["The ",(0,a.jsx)(r.code,{children:"triangle_index_buffer"})," rasterizes the index of the triangle to the buffer, or -1 for empty pixels."]}),"\n",(0,a.jsxs)(r.li,{children:["The ",(0,a.jsx)(r.code,{children:"surface_normals_buffer"})," rasterizes the direction of the surface normal in eye coordinates, or all zeros if empty pixels."]}),"\n"]}),"\n",(0,a.jsx)(r.p,{children:"These last two buffers can be used for things like per-part segmentation (use the rendered vertex indices to look up into a vertex index to part ID mapping)."}),"\n",(0,a.jsx)(r.pre,{children:(0,a.jsx)(r.code,{className:"language-python",children:"# Default index buffer is set to -1 everywhere (this is because vertex\n# indices start at 0)\nvertex_index_buffer = pym_renderer.create_index_buffer(camera)\ntriangle_index_buffer = pym_renderer.create_index_buffer(camera)\nnormals_buffer = pym_renderer.create_rgb_buffer(camera)\n\npym_renderer.rasterize_character(character, skel_state, camera,\n    z_buffer=z_buffer,\n    surface_normals_buffer=normals_buffer,\n    vertex_index_buffer=vertex_index_buffer,\n    triangle_index_buffer=triangle_index_buffer,\n)\n\n# Generate some random colors:\nrandom_colors = torch.rand(\n    max(triangles.shape[0], vertices.shape[0]), 3, dtype=torch.float32\n)\n# Need to shift by 1 since empty pixels are set to -1 (torch tensor indexing doesn't\n# appear to support -1).\ntriangle_colors = random_colors[triangle_index_buffer.flatten() + 1, :].reshape(\n    rgb_buffer.shape\n)\nvertex_colors = random_colors[vertex_index_buffer.flatten() + 1, :].reshape(\n    rgb_buffer.shape\n)\n"})}),"\n",(0,a.jsx)(r.p,{children:"From left: RGB buffer, normals buffer, triangle index buffer, vertex index buffer (notice the Voronoi regions)."}),"\n",(0,a.jsx)(r.h3,{id:"antialiasing",children:"Antialiasing"}),"\n",(0,a.jsxs)(r.p,{children:["The rasterizer doesn't do any antialiasing, so you may see some jagged edges in your renders. This will probably be less important for meshes but is going to be particularly noticeable for thin structures like lines or thin cylinders. This is easy to fix by supersampling the image, just create a larger camera using ",(0,a.jsx)(r.code,{children:"camera.upsample()"})," and then downsample at the end."]}),"\n",(0,a.jsx)(r.pre,{children:(0,a.jsx)(r.code,{className:"language-python",children:"import pymomentum.renderer as pym_renderer\nsup_samp: int = 2\ncam_supersample = cam.upsample(sup_samp)\nz_buffer = pym_renderer.create_z_buffer(cam_supersample)\nrgb_buffer = pym_renderer.create_rgb_buffer(cam_supersample)\n\n# render\npym_renderer.rasterize_mesh(...)\n\noutput_image = np.zeros(shape=(cam.image_height, cam.image_width, 3)\n# Alpha_matte function knows how to handle alpha with upsampled cameras (will\n# correctly blend along edges using the averaged alpha).\npym_renderer.alpha_matte(z_buffer, rgb_buffer, output_image)\n"})}),"\n",(0,a.jsx)(r.p,{children:"No supersampling vs with supersampling provides significantly better visual quality."})]})}function c(e={}){const{wrapper:r}={...(0,i.R)(),...e.components};return r?(0,a.jsx)(r,{...e,children:(0,a.jsx)(h,{...e})}):h(e)}}}]);