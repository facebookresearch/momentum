"use strict";(self.webpackChunkstaticdocs_starter=self.webpackChunkstaticdocs_starter||[]).push([[4745],{42993:(e,n,i)=>{i.r(n),i.d(n,{assets:()=>d,contentTitle:()=>o,default:()=>h,frontMatter:()=>t,metadata:()=>c,toc:()=>l});var s=i(74848),r=i(28453);const t={sidebar_position:2},o="Design Decisions",c={id:"developer_guide/design_decisions",title:"Design Decisions",description:"NumPy vs PyTorch Tensors",source:"@site/docs_python/03_developer_guide/02_design_decisions.md",sourceDirName:"03_developer_guide",slug:"/developer_guide/design_decisions",permalink:"/momentum/pymomentum/developer_guide/design_decisions",draft:!1,unlisted:!1,editUrl:"https://github.com/facebookresearch/momentum/edit/main/momentum/website/docs_python/03_developer_guide/02_design_decisions.md",tags:[],version:"current",sidebarPosition:2,frontMatter:{sidebar_position:2},sidebar:"tutorialSidebar",previous:{title:"Development Environment",permalink:"/momentum/pymomentum/developer_guide/development_environment"},next:{title:"PyPI Publishing Guide",permalink:"/momentum/pymomentum/developer_guide/pypi_publishing"}},d={},l=[{value:"NumPy vs PyTorch Tensors",id:"numpy-vs-pytorch-tensors",level:2},{value:"Why the Mixed Approach?",id:"why-the-mixed-approach",level:3},{value:"Current State",id:"current-state",level:3},{value:"Future Directions",id:"future-directions",level:3}];function a(e){const n={code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",li:"li",p:"p",strong:"strong",ul:"ul",...(0,r.R)(),...e.components};return(0,s.jsxs)(s.Fragment,{children:[(0,s.jsx)(n.header,{children:(0,s.jsx)(n.h1,{id:"design-decisions",children:"Design Decisions"})}),"\n",(0,s.jsx)(n.h2,{id:"numpy-vs-pytorch-tensors",children:"NumPy vs PyTorch Tensors"}),"\n",(0,s.jsx)(n.p,{children:"PyMomentum mixes NumPy arrays and PyTorch tensors throughout the codebase. This explains the reasoning and current status."}),"\n",(0,s.jsx)(n.h3,{id:"why-the-mixed-approach",children:"Why the Mixed Approach?"}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"Old Reason (Historical):"})}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Auto-conversion from Eigen types to NumPy arrays works natively in pybind11"}),"\n",(0,s.jsx)(n.li,{children:"Can use Python buffer interface to wrap data without copies"}),"\n",(0,s.jsx)(n.li,{children:'PyTorch tensors are "more painful to work with" in pybind11'}),"\n",(0,s.jsx)(n.li,{children:"Used PyTorch only where differentiability was needed"}),"\n"]}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"Internal Convention:"})}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"PyTorch tensor = differentiable operation"}),"\n",(0,s.jsx)(n.li,{children:"NumPy array = non-differentiable operation"}),"\n"]}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"New Reason (Current):"}),"\nThe dependence on aten makes building Python code that uses torch.Tensor somewhat challenging compared to code that uses the basic buffer interface:"]}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Blocking pymomentum usage in downstream projects"}),"\n",(0,s.jsx)(n.li,{children:"Preventing demonstration projects"}),"\n",(0,s.jsx)(n.li,{children:"Causing various compatibility problems"}),"\n"]}),"\n",(0,s.jsx)(n.h3,{id:"current-state",children:"Current State"}),"\n",(0,s.jsx)(n.p,{children:"The codebase is mixed which can be confusing because we're between approaches. Discussions are happening around:"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:["Stripping all PyTorch from ",(0,s.jsx)(n.code,{children:"pymomentum.geometry"})]}),"\n",(0,s.jsxs)(n.li,{children:["Isolating differentiability (and hence PyTorch dependencies) in specific libraries, such as ",(0,s.jsx)(n.code,{children:"diff_solver"})]}),"\n",(0,s.jsxs)(n.li,{children:["Using ",(0,s.jsx)(n.code,{children:"GPU_character"})," for ML workloads instead"]}),"\n"]}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"Examples:"})}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.code,{children:"solver2"})," uses NumPy arrays (not differentiable, PyTorch-independent)"]}),"\n",(0,s.jsx)(n.li,{children:"Batching support being removed from non-ML contexts (like rendering)"}),"\n",(0,s.jsx)(n.li,{children:"Rendering code will strip batch support for open sourcing"}),"\n"]}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"PyTorch advantages being lost:"})}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Extra tensor manipulation functionality"}),"\n",(0,s.jsx)(n.li,{children:"Batching/unbatching support"}),"\n",(0,s.jsx)(n.li,{children:"But interfaces become more confusing for non-ML use"}),"\n"]}),"\n",(0,s.jsx)(n.h3,{id:"future-directions",children:"Future Directions"}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Potential approach:"})," If you need batching + differentiability + GPU support, use ",(0,s.jsx)(n.code,{children:"GPU_character"})," instead of pymomentum. This would let us:"]}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Strip enormous amounts of code from pymomentum"}),"\n",(0,s.jsx)(n.li,{children:"Fix build system issues"}),"\n",(0,s.jsx)(n.li,{children:"Simplify interfaces"}),"\n"]}),"\n",(0,s.jsxs)(n.p,{children:["(Obviously could only happen after open sourcing ",(0,s.jsx)(n.code,{children:"GPU_character"}),")"]}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Current recommendation:"})," Accept manual conversions between NumPy/PyTorch until the architecture stabilizes."]})]})}function h(e={}){const{wrapper:n}={...(0,r.R)(),...e.components};return n?(0,s.jsx)(n,{...e,children:(0,s.jsx)(a,{...e})}):a(e)}}}]);